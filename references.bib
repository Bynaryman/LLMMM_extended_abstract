@misc{luccioni2022estimating,
      title={Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model},
      author={Alexandra Sasha Luccioni and Sylvain Viguier and Anne-Laure Ligozat},
      year={2022},
      eprint={2211.02001},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ajayi2019toward,
  title={Toward an open-source digital flow: First learnings from the openroad project},
  author={Ajayi, Tutu and Chhabria, Vidya A and Foga{\c{c}}a, Mateus and Hashemi, Soheil and Hosny, Abdelrahman and Kahng, Andrew B and Kim, Minsoo and Lee, Jeongsup and Mallappa, Uday and Neseem, Marina and others},
  booktitle={Proceedings of the 56th Annual Design Automation Conference 2019},
  pages={1--4},
  year={2019}
}

@techreport{micikevicius_fp8_2022,
	title = {{FP8} {Formats} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2209.05433},
	abstract = {FP8 is a natural progression for accelerating deep learning training inference beyond the 16-bit formats common in modern processors. In this paper we propose an 8-bit ﬂoating point (FP8) binary interchange format consisting of two encodings - E4M3 (4-bit exponent and 3-bit mantissa) and E5M2 (5-bit exponent and 2-bit mantissa). While E5M2 follows IEEE 754 conventions for representatio of special values, E4M3’s dynamic range is extended by not representing inﬁnities and having only one mantissa bit-pattern for NaNs. We demonstrate the efﬁcacy of the FP8 format on a variety of image and language tasks, effectively matching the result quality achieved by 16-bit training sessions. Our study covers the main modern neural network architectures - CNNs, RNNs, and Transformer-based models, leaving all the hyperparameters unchanged from the 16-bit baseline training sessions. Our training experiments include large, up to 175B parameter, language models. We also examine FP8 post-training-quantization of language models trained using 16-bit formats that resisted ﬁxed point int8 quantization.},
	language = {en},
	number = {arXiv:2209.05433},
	institution = {arXiv},
	author = {Micikevicius, Paulius and Stosic, Dusan and Burgess, Neil and Cornea, Marius and Dubey, Pradeep and Grisenthwaite, Richard and Ha, Sangwon and Heinecke, Alexander and Judd, Patrick and Kamalu, John and Mellempudi, Naveen and Oberman, Stuart and Shoeybi, Mohammad and Siu, Michael and Wu, Hao},
	month = sep,
	year = {2022},
	note = {arXiv:2209.05433 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{gustafson_beating_2017,
	title = {Beating {Floating} {Point} at its {Own} {Game}: {Posit} {Arithmetic}},
	volume = {4},
	copyright = {Copyright (c)},
	issn = {2313-8734},
	shorttitle = {Beating {Floating} {Point} at its {Own} {Game}},
	url = {https://superfri.org/index.php/superfri/article/view/137},
	doi = {10.14529/jsfi170206},
	abstract = {A new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers (floats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and “Not-a-Number” (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality. A comprehensive series of benchmarks compares floats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than “approximate computing” methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than floats of the same size; in some cases, a 32-bit posit may safely replace a 64-bit float. In other words, posits beat floats at their own game.},
	language = {en},
	number = {2},

	journal = {Supercomputing Frontiers and Innovations},
	author = {Gustafson, John L. and Yonemoto, Isaac T.},
	month = apr,
	year = {2017},
	note = {Number: 2},
	pages = {71--86},
}
@misc{ma2024era,
      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits},
      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
      year={2024},
      eprint={2402.17764},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
