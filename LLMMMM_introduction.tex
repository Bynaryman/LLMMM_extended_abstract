\subsection{Introduction}
\label{sec:introduction}

GPT transformers are useful for various applications, offering significant advancements in natural language processing tasks.
However, their operational costs are substantial has shown in prior work which highlights the financial implications of deploying these models~\cite{luccioni2022estimating}.

Essentially, matrix-matrix multiplications (MMM), with their intensive data movement and manipulation of arithmetic weights, underscore the computational demands of these architectures.
Naturally, these observations are also found in recent efforts within the research community, which have concentrated on devising specialized formats and algorithms aimed at mitigating these costs.
These innovations include reducing bit-width exemplified by Machine Learning eXchange (MLX) formats (essentially small floats), specialized hardware such as TPUs' systolic arrays, model pruning of up to 40\%, and more recently, ternary and binary LLMs (see BitNets~\cite{ma2024era}).

We introduce a generator of ASIC kernels agnostic to the PDK of MMM units for emerging and small floating-point formats, followed by the evaluation of such units.
Concretely, our contributions include the automated generation of circuits for any floating-point format with automated pipelining, a systolic array architecture proposalâ€”these two combined form the foundation of MMM units, a framework to automate the translation from high-level language (Python) to silicon for such matrices, the generation of $4 \text{ arithmetic formats} \times 2 \text{ accumulator configurations} \times 4 \text{ PDKs} = 32$ chips, and their performance and efficiency evaluation, all provided as open source.
